{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用Python进行简单的文本相似度分析\n",
    "最近一个Webshell的检测的项目中，采用了一些静态分析的方法，其中也涉及一些文本匹配的功能，故以此文作为这方面材料的收集和记录，以便将来回顾。\n",
    "\n",
    "## 文本相似度算法\n",
    "### 子序列和子字符串\n",
    "这个系列问题包含这么几种：最大子序列，最长递增子序列，最长公共子串，最长公共子序列。\n",
    "\n",
    "几个子问题都可以用动态规划的思路求解，对于长度为i,j的两个字符串，使用m[i][j]矩阵存放中间结果。\n",
    "\n",
    "更详细的算法可以参考此文档：\n",
    "* [最大子序列、最长递增子序列、最长公共子串、最长公共子序列、字符串编辑距离][1]\n",
    "\n",
    "### 字符串编辑巨鹿\n",
    "精确计算两个字符串的编辑距离，可以使用经典的动态规划思路。\n",
    "\n",
    "这里来看下如何判断字符串A与B的编辑是否>N？这样我们就可以比较两个字符串的相似度了。\n",
    "\n",
    "可以构建一个编辑距离自动机（超酷算法：[Levenshtein自动机][2]）,把测试字符集合输入自动机进行判断。\n",
    "\n",
    "### 向量近似度\n",
    "使用TF-IDF计算出文本中词和词频集合，把该集合作一个向量，比较不同集合向量在线性空间中的相似度。如：余弦距离，欧氏距离，概率分布距离（K-L距离）等。\n",
    "\n",
    "更详细的介绍可参考一下文档：\n",
    "* [TF-IDF与余弦相似性的应用（二）：找出相似文章][3]\n",
    "* [常用的相似度计算方法：Python实现][4]\n",
    "\n",
    "### SimHash\n",
    "Simhash算法的主要思想是降维，将高维的特征向量映射成一个f-bit的指纹（fingerprint）,通过比较两篇文章的f-bit指纹的Hamming Distance来确定文章是否重复或高度近似。\n",
    "\n",
    "主要分以下几步：\n",
    "1. 抽取文本中的关键词及其权重\n",
    "2. 对关键词取传统hash，并与权重叠加，算出文本的fingerprint值\n",
    "3. 计算出两个文本之间的fingerprint值的海明距离\n",
    "\n",
    "更详细的介绍可参考：\n",
    "* [文档去重算法：SimHash和MinHash][5]\n",
    "\n",
    "[1]: http://www.cnblogs.com/zhangchaoyang/articles/2012070.html\n",
    "[2]: http://blog.jobbole.com/80659/\n",
    "[3]: http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html\n",
    "[4]: https://blog.csdn.net/u013393647/article/details/46754055\n",
    "[5]: https://blog.csdn.net/heiyeshuwu/article/details/44117473"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面我们主要以Python的gensim模块展开介绍和实操训练\n",
    "\n",
    "### 环境和相关工具\n",
    "* Python3.5.2\n",
    "* jupyter notebook\n",
    "* jieba\n",
    "* gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先引入分词的API库jieba，文本相似度库gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剔除停用词“stop-word”，用来剔除不需要的干扰:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['，', '。']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入几个最简单的目标文档："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "docs.append('打球不练腿，早晚得羊尾')\n",
    "docs.append('打完球，去吃小尾羊')\n",
    "docs.append('打球，之后写作业')\n",
    "docs.append('先加班，再打球')\n",
    "docs.append('早晚打球，就会一点点进步')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_list = []\n",
    "for doc in docs:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    doc_list = list(set(doc_list) - set(stop_words))\n",
    "    all_doc_list.append(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['打球', '腿', '羊尾', '早晚', '不练', '得'],\n",
       " ['完球', '去', '吃', '打', '小尾羊'],\n",
       " ['写', '作业', '打球', '之后'],\n",
       " ['再', '打球', '加班', '先'],\n",
       " ['就', '一点点', '打球', '会', '早晚', '进步']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把待测试的文本进行分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = '打球若练腿，不可能羊尾'\n",
    "doc_test_list = [word for word in jieba.cut(doc_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['打球', '若练', '腿', '，', '不', '可能', '羊尾']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_test_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "制作语料库，首先用dictionary方法获取词袋（bag-of-words）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(all_doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词袋用数字对所有词进行了编号："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 17, 12, 2, 19, 5, 18, 16, 4, 20, 15, 6, 7, 0, 9, 8, 3, 1, 11, 10, 14]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编号与词之间的对应关系："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'一点点': 17,\n",
       " '不练': 0,\n",
       " '之后': 11,\n",
       " '会': 18,\n",
       " '作业': 12,\n",
       " '先': 14,\n",
       " '再': 15,\n",
       " '写': 13,\n",
       " '加班': 16,\n",
       " '去': 6,\n",
       " '吃': 7,\n",
       " '完球': 8,\n",
       " '小尾羊': 9,\n",
       " '就': 19,\n",
       " '得': 1,\n",
       " '打': 10,\n",
       " '打球': 2,\n",
       " '早晚': 3,\n",
       " '羊尾': 4,\n",
       " '腿': 5,\n",
       " '进步': 20}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用doc2bow制作语料库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in all_doc_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "语料库是一组向量，向量中的元素是一个二元组（编号，频次数），对应分词后的文档中的每一个词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
       " [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
       " [(2, 1), (11, 1), (12, 1), (13, 1)],\n",
       " [(2, 1), (14, 1), (15, 1), (16, 1)],\n",
       " [(2, 1), (3, 1), (17, 1), (18, 1), (19, 1), (20, 1)]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用同样的方法，把测试文档也转换为二元组的向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test_vec = dictionary.doc2bow(doc_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1), (4, 1), (5, 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_test_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相似度分析，使用TF-IDF模型对语料库建模："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对每个目标文档，分析测试文档的相似度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.68183553),\n",
       " (2, 0.007785392),\n",
       " (3, 0.007785392),\n",
       " (4, 0.0064910594),\n",
       " (1, 0.0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary.keys()))\n",
    "sim = index[tfidf[doc_test_vec]]\n",
    "sorted(enumerate(sim), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从以上结果中，可以看出，待测文本和第一个文本匹配对最高，符合真实情况。\n",
    "\n",
    "### 文本相似度分析的步骤\n",
    "* 读取文档\n",
    "* 对要计算的多篇文档进行分词\n",
    "* 对文档进行整理成指定格式，方便后续进行计算\n",
    "* 计算出词语的词频\n",
    "* 【可选】对词频低的词语进行过滤\n",
    "* 建立语料库词典\n",
    "* 加载要对比的文档\n",
    "* 将要对比的文档通过doc2bow转化为词袋模型\n",
    "* 对词袋模型进行进一步处理，得到新语料库\n",
    "* 将新语料库通过tfidfmodel进行处理，得到tfidf\n",
    "* 通过token2id得到特征数 12、稀疏矩阵相似度，从而建立索引 13、得到最终相似度结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 附上更完整示例\n",
    "测试Python源文件相似度，其中包含主脚本：mydoc_sim.py和其他两个用于对比的脚本程序文件：test_sim_1.py，doc_sim.py，待比对文件：test_sim.py\n",
    "\n",
    "mydoc_sim.py\n",
    "\n",
    "```python\n",
    "# coding: utf-8\n",
    "import jieba\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "\n",
    "# Stop words将会用作过滤分词\n",
    "stop_words = ['，', '。']\n",
    "\n",
    "test_data_1 = open('test_sim.py').read()\n",
    "data = open('doc_sim.py').read()\n",
    "data2 = open('mydoc_sim.py').read()\n",
    "data3 = open('test_sim_1.py').read()\n",
    "\n",
    "all_doc = []\n",
    "all_doc.append(data)\n",
    "all_doc.append(data2)\n",
    "all_doc.append(data3)\n",
    "\n",
    "# 分词\n",
    "all_doc_list = []\n",
    "for doc in all_doc:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    doc_list = list(set(doc_list) - set(stop_words))\n",
    "    all_doc_list.append(doc_list)\n",
    "\n",
    "doc_test_list = [word for word in jieba.cut(test_data_1)]\n",
    "doc_test_list = list(set(doc_test_list) - set(stop_words))\n",
    "\n",
    "# 制作词袋: bag-of-words\n",
    "dictionary = corpora.Dictionary(all_doc_list)\n",
    "# print(dictionary.keys())\n",
    "# print(dictionary.token2id)\n",
    "\n",
    "# 使用doc2bow制作语料库\n",
    "corpus = [dictionary.doc2bow(doc) for doc in all_doc_list]\n",
    "\n",
    "doc_test_vec = dictionary.doc2bow(doc_test_list)\n",
    "\n",
    "# 相似度分析，使用TF-IDF对语料库建模\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "# 分析相似度\n",
    "index = similarities.SparseMatrixSimilarity(\n",
    "    tfidf[corpus], num_features=len(dictionary.keys())\n",
    ")\n",
    "sim = index[tfidf[doc_test_vec]]\n",
    "print(sorted(enumerate(sim), key=lambda item: -item[1]))\n",
    "```\n",
    "\n",
    "test_sim.py\n",
    "\n",
    "```python\n",
    "# coding: utf-8\n",
    "import jieba\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "doc0 = \"我不喜欢上海\"\n",
    "doc1 = \"上海是一个好地方\"\n",
    "doc2 = \"北京是一个好地方\"\n",
    "doc3 = \"上海好吃的在哪里\"\n",
    "doc4 = \"上海好玩的在哪里\"\n",
    "doc5 = \"上海是好地方\"\n",
    "doc6 = \"上海路和上海人\"\n",
    "doc7 = \"喜欢小吃\"\n",
    "doc_test=\"我喜欢上海的小吃\"\n",
    "\n",
    "all_doc = []\n",
    "all_doc.append(doc0)\n",
    "all_doc.append(doc1)\n",
    "all_doc.append(doc2)\n",
    "all_doc.append(doc3)\n",
    "all_doc.append(doc4)\n",
    "all_doc.append(doc5)\n",
    "all_doc.append(doc6)\n",
    "all_doc.append(doc7)\n",
    "\n",
    "all_doc_list = []\n",
    "for doc in all_doc:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    all_doc_list.append(doc_list)\n",
    "\n",
    "print(all_doc_list)\n",
    "\n",
    "doc_test_list = [word for word in jieba.cut(doc_test)]\n",
    "\n",
    "dictionary = corpora.Dictionary(all_doc_list)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in all_doc_list]\n",
    "\n",
    "doc_test_vec = dictionary.doc2bow(doc_test_list)\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary.keys()))\n",
    "sim = index[tfidf[doc_test_vec]]\n",
    "\n",
    "print(sorted(enumerate(sim), key=lambda item: -item[1]))\n",
    "```\n",
    "\n",
    "doc_sim.py\n",
    "\n",
    "```python\n",
    "#coding:utf-8\n",
    "#使用docsim方法：doc2bow、similarities判断相似性\n",
    "from gensim import models,corpora,similarities\n",
    "import jieba.posseg as pseg\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def a_sub_b(a,b):\n",
    "    ret = []\n",
    "    for el in a:\n",
    "        if el not in b:\n",
    "            ret.append(el)\n",
    "    return ret\n",
    "\n",
    "#读取文件\n",
    "raw_documents=[]\n",
    "walk = os.walk(os.path.realpath(\"/root/python_scripts/doc_similarity/doc/\"))\n",
    "for root, dirs, files in walk:\n",
    "    for name in files:\n",
    "        f = open(os.path.join(root, name), 'r')\n",
    "    raw = os.path.join(root, name)+ \" \"\n",
    "    raw += f.read()\n",
    "    raw_documents.append(raw)\n",
    "stop = [line.strip() for line in open('stopword.txt').readlines() ]\n",
    "#创建语料库\n",
    "corpora_documents = []\n",
    "for item_text in raw_documents:\n",
    "    item_str=[]\n",
    "    item= (pseg.cut(item_text)) #使用jieba分词\n",
    "    for i in list(item):\n",
    "        item_str.append(i.word)\n",
    "    item_str=a_sub_b(item_str,list(stop))\n",
    "    print(item_str)\n",
    "    #sys.exit(0)\n",
    "    corpora_documents.append(item_str)\n",
    "\n",
    "# 生成字典和向量语料\n",
    "dictionary = corpora.Dictionary(corpora_documents) #把所有单词取一个set，并对set中每一个单词分配一个id号的map\n",
    "corpus = [dictionary.doc2bow(text) for text in corpora_documents]  #把文档doc变成一个稀疏向量，[(0,1),(1,1)]表明id为0,1的词出现了1次，其他未出现。\n",
    "similarity = similarities.Similarity('-Similarity-index', corpus, num_features=999999999)\n",
    "\n",
    "test_data_1 = '本报讯 全球最大个人电脑制造商戴尔公司８日说，由于市场竞争激烈，以及定价策略不当，该公司今年第一季度盈利预计有所下降。'\\\n",
    "'消息发布之后，戴尔股价一度下跌近６％，创下一年来的新低。戴尔公司估计，其第一季度收入约为１４２亿美元，每股收益３３美分。此前公司预测当季收入为１４２亿至１４６亿美元，'\\\n",
    "'每股收益３６至３８美分，而分析师平均预测戴尔同期收入为１４５．２亿美元，每股收益３８美分。为抢夺失去的市场份额，戴尔公司一些产品打折力度很大。戴尔公司首席执行官凯文·罗林斯在一份声明中说，公司在售后服务和产品质量方面一直在投资，同时不断下调价格。戴尔公司将于５月１８日公布第一季度的财报。'\n",
    "test_cut = pseg.cut(test_data_1)\n",
    "test_cut_raw_1=[]\n",
    "for i in list(test_cut):\n",
    "    test_cut_raw_1.append(i.word)\n",
    "test_corpus_1 = dictionary.doc2bow(test_cut_raw_1)\n",
    "similarity.num_best = 5\n",
    "print(similarity[test_corpus_1])  # 返回最相似的样本材料,(index_of_document, similarity) tuples\n",
    "for i in similarity[test_corpus_1]:\n",
    "    sim=\"\"\n",
    "    print('################################')\n",
    "    print(i[0])\n",
    "    for j in corpora_documents[i[0]]:\n",
    "        sim+=j\n",
    "print(sim)\n",
    "```\n",
    "\n",
    "test_sim_1.py\n",
    "\n",
    "```python\n",
    "# coding: utf-8\n",
    "import jieba\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "doc0 = \"我不喜欢上海\"\n",
    "doc1 = \"上海是一个好地方\"\n",
    "doc2 = \"北京是一个好地方\"\n",
    "doc3 = \"上海好吃的在哪里\"\n",
    "doc6 = \"上海路和上海人\"\n",
    "doc7 = \"喜欢小吃\"\n",
    "doc_test=\"我喜欢上海的小吃\"\n",
    "\n",
    "all_document = []\n",
    "all_document.append(doc0)\n",
    "all_document.append(doc1)\n",
    "all_document.append(doc2)\n",
    "all_document.append(doc3)\n",
    "all_document.append(doc6)\n",
    "all_document.append(doc7)\n",
    "\n",
    "all_document_list = []\n",
    "for doc in all_document:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    all_document_list.append(doc_list)\n",
    "\n",
    "print(all_document_list)\n",
    "\n",
    "doc_test_l = [word for word in jieba.cut(doc_test)]\n",
    "\n",
    "dictionary = corpora.Dictionary(all_document_list)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in all_document_list]\n",
    "\n",
    "doc_test_vec = dictionary.doc2bow(doc_test_l)\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "index_value = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary.keys()))\n",
    "sim = index_value[tfidf[doc_test_vec]]\n",
    "\n",
    "print(sorted(enumerate(sim), key=lambda item: -item[1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
