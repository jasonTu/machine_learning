{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用Python进行简单的文本相似度分析\n",
    "最近一个Webshell的检测的项目中，采用了一些静态分析的方法，其中也涉及一些文本匹配的功能，故以此文作为这方面材料的收集和记录，以便将来回顾。\n",
    "\n",
    "## 文本相似度算法\n",
    "### 子序列和子字符串\n",
    "这个系列问题包含这么几种：最大子序列，最长递增子序列，最长公共子串，最长公共子序列。\n",
    "\n",
    "几个子问题都可以用动态规划的思路求解，对于长度为i,j的两个字符串，使用m[i][j]矩阵存放中间结果。\n",
    "\n",
    "更详细的算法可以参考此文档：\n",
    "* [最大子序列、最长递增子序列、最长公共子串、最长公共子序列、字符串编辑距离][1]\n",
    "\n",
    "### 字符串编辑距离\n",
    "精确计算两个字符串的编辑距离，可以使用经典的动态规划思路。\n",
    "\n",
    "这里来看下如何判断字符串A与B的编辑是否>N？这样我们就可以比较两个字符串的相似度了。\n",
    "\n",
    "可以构建一个编辑距离自动机（超酷算法：[Levenshtein自动机][2]）,把测试字符集合输入自动机进行判断。\n",
    "\n",
    "### 向量近似度\n",
    "使用TF-IDF计算出文本中词和词频集合，把该集合作一个向量，比较不同集合向量在线性空间中的相似度。如：余弦距离，欧氏距离，概率分布距离（K-L距离）等。\n",
    "\n",
    "更详细的介绍可参考一下文档：\n",
    "* [TF-IDF与余弦相似性的应用（二）：找出相似文章][3]\n",
    "* [常用的相似度计算方法：Python实现][4]\n",
    "\n",
    "### SimHash\n",
    "Simhash算法的主要思想是降维，将高维的特征向量映射成一个f-bit的指纹（fingerprint）,通过比较两篇文章的f-bit指纹的Hamming Distance来确定文章是否重复或高度近似。\n",
    "\n",
    "主要分以下几步：\n",
    "1. 抽取文本中的关键词及其权重\n",
    "2. 对关键词取传统hash，并与权重叠加，算出文本的fingerprint值\n",
    "3. 计算出两个文本之间的fingerprint值的海明距离\n",
    "\n",
    "更详细的介绍可参考：\n",
    "* [文档去重算法：SimHash和MinHash][5]\n",
    "\n",
    "[1]: http://www.cnblogs.com/zhangchaoyang/articles/2012070.html\n",
    "[2]: http://blog.jobbole.com/80659/\n",
    "[3]: http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html\n",
    "[4]: https://blog.csdn.net/u013393647/article/details/46754055\n",
    "[5]: https://blog.csdn.net/heiyeshuwu/article/details/44117473"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面我们主要以Python的gensim模块展开介绍和实操训练\n",
    "\n",
    "### 环境和相关工具\n",
    "* Python3.5.2\n",
    "* jupyter notebook\n",
    "* jieba\n",
    "* gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先引入分词的API库jieba，文本相似度库gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剔除停用词“stop-word”，用来剔除不需要的干扰:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['，', '。']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入几个最简单的目标文档："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "docs.append('打球不练腿，早晚得羊尾')\n",
    "docs.append('早晚打球后，去吃小尾羊')\n",
    "docs.append('打球不，吃羊尾')\n",
    "docs.append('先练球，再打球')\n",
    "docs.append('早晚打球练腿，就会一点点进步')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_list = []\n",
    "for doc in docs:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    doc_list = list(set(doc_list) - set(stop_words))\n",
    "    all_doc_list.append(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['腿', '打球', '早晚', '不练', '羊尾', '得'],\n",
       " ['去', '打球', '早晚', '吃', '小尾羊', '后'],\n",
       " ['羊尾', '不', '吃', '打球'],\n",
       " ['先', '练球', '再', '打球'],\n",
       " ['打球', '早晚', '一点点', '会', '就', '练腿', '进步']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把待测试的文本进行分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = '先打球练腿，不可能羊尾'\n",
    "doc_test_list = [word for word in jieba.cut(doc_test)]\n",
    "doc_test_list = list(set(doc_test_list) - set(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['不', '打球', '先', '可能', '羊尾', '练腿']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_test_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "制作语料库，首先用dictionary方法获取词袋（bag-of-words）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(all_doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词袋用数字对所有词进行了编号："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 10, 2, 5, 3, 11, 12, 14, 18, 16, 0, 7, 4, 1, 9, 15, 13, 17, 8]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编号与词之间的对应关系："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'一点点': 14,\n",
       " '不': 10,\n",
       " '不练': 0,\n",
       " '会': 15,\n",
       " '先': 11,\n",
       " '再': 12,\n",
       " '去': 6,\n",
       " '吃': 7,\n",
       " '后': 8,\n",
       " '小尾羊': 9,\n",
       " '就': 16,\n",
       " '得': 1,\n",
       " '打球': 2,\n",
       " '早晚': 3,\n",
       " '练球': 13,\n",
       " '练腿': 17,\n",
       " '羊尾': 4,\n",
       " '腿': 5,\n",
       " '进步': 18}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用doc2bow制作语料库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in all_doc_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "语料库是一组向量，向量中的元素是一个二元组（编号，频次数），对应分词后的文档中的每一个词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
       " [(2, 1), (3, 1), (6, 1), (7, 1), (8, 1), (9, 1)],\n",
       " [(2, 1), (4, 1), (7, 1), (10, 1)],\n",
       " [(2, 1), (11, 1), (12, 1), (13, 1)],\n",
       " [(2, 1), (3, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用同样的方法，把测试文档也转换为二元组的向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test_vec = dictionary.doc2bow(doc_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1), (4, 1), (10, 1), (11, 1), (17, 1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_test_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相似度分析，使用TF-IDF模型对语料库建模："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对每个目标文档，分析测试文档的相似度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.5656904), (3, 0.31666526), (4, 0.24285358), (0, 0.09606325), (1, 0.0)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary.keys()))\n",
    "sim = index[tfidf[doc_test_vec]]\n",
    "sorted(enumerate(sim), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09606325, 0.        , 0.5656904 , 0.31666526, 0.24285358],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(sim, index=['one', 'two', 'three', 'four', 'five'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7768c02b38>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从以上结果中，可以看出，待测文本和第三个文本匹配对最高，符合真实情况（样本空间有限）。\n",
    "\n",
    "### 文本相似度分析的步骤\n",
    "* 读取文档\n",
    "* 对要计算的多篇文档进行分词\n",
    "* 对文档进行整理成指定格式，方便后续进行计算\n",
    "* 计算出词语的词频\n",
    "* 【可选】对词频低的词语进行过滤\n",
    "* 建立语料库词典\n",
    "* 加载要对比的文档\n",
    "* 将要对比的文档通过doc2bow转化为词袋模型\n",
    "* 对词袋模型进行进一步处理，得到新语料库\n",
    "* 将新语料库通过tfidfmodel进行处理，得到tfidf\n",
    "* 通过token2id得到特征数 12、稀疏矩阵相似度，从而建立索引 13、得到最终相似度结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 附上更完整示例\n",
    "测试Python源文件相似度，其中包含主脚本：mydoc_sim.py和其他两个用于对比的脚本程序文件：test_sim_1.py，doc_sim.py，待比对文件：test_sim.py\n",
    "\n",
    "mydoc_sim.py\n",
    "\n",
    "```python\n",
    "# coding: utf-8\n",
    "import jieba\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "\n",
    "# Stop words将会用作过滤分词\n",
    "stop_words = ['，', '。']\n",
    "\n",
    "test_data_1 = open('test_sim.py').read()\n",
    "data = open('doc_sim.py').read()\n",
    "data2 = open('mydoc_sim.py').read()\n",
    "data3 = open('test_sim_1.py').read()\n",
    "\n",
    "all_doc = []\n",
    "all_doc.append(data)\n",
    "all_doc.append(data2)\n",
    "all_doc.append(data3)\n",
    "\n",
    "# 分词\n",
    "all_doc_list = []\n",
    "for doc in all_doc:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    doc_list = list(set(doc_list) - set(stop_words))\n",
    "    all_doc_list.append(doc_list)\n",
    "\n",
    "doc_test_list = [word for word in jieba.cut(test_data_1)]\n",
    "doc_test_list = list(set(doc_test_list) - set(stop_words))\n",
    "\n",
    "# 制作词袋: bag-of-words\n",
    "dictionary = corpora.Dictionary(all_doc_list)\n",
    "# print(dictionary.keys())\n",
    "# print(dictionary.token2id)\n",
    "\n",
    "# 使用doc2bow制作语料库\n",
    "corpus = [dictionary.doc2bow(doc) for doc in all_doc_list]\n",
    "\n",
    "doc_test_vec = dictionary.doc2bow(doc_test_list)\n",
    "\n",
    "# 相似度分析，使用TF-IDF对语料库建模\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "# 分析相似度\n",
    "index = similarities.SparseMatrixSimilarity(\n",
    "    tfidf[corpus], num_features=len(dictionary.keys())\n",
    ")\n",
    "sim = index[tfidf[doc_test_vec]]\n",
    "print(sorted(enumerate(sim), key=lambda item: -item[1]))\n",
    "```\n",
    "\n",
    "test_sim.py\n",
    "\n",
    "```python\n",
    "# coding: utf-8\n",
    "import jieba\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "doc0 = \"我不喜欢上海\"\n",
    "doc1 = \"上海是一个好地方\"\n",
    "doc2 = \"北京是一个好地方\"\n",
    "doc3 = \"上海好吃的在哪里\"\n",
    "doc4 = \"上海好玩的在哪里\"\n",
    "doc5 = \"上海是好地方\"\n",
    "doc6 = \"上海路和上海人\"\n",
    "doc7 = \"喜欢小吃\"\n",
    "doc_test=\"我喜欢上海的小吃\"\n",
    "\n",
    "all_doc = []\n",
    "all_doc.append(doc0)\n",
    "all_doc.append(doc1)\n",
    "all_doc.append(doc2)\n",
    "all_doc.append(doc3)\n",
    "all_doc.append(doc4)\n",
    "all_doc.append(doc5)\n",
    "all_doc.append(doc6)\n",
    "all_doc.append(doc7)\n",
    "\n",
    "all_doc_list = []\n",
    "for doc in all_doc:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    all_doc_list.append(doc_list)\n",
    "\n",
    "print(all_doc_list)\n",
    "\n",
    "doc_test_list = [word for word in jieba.cut(doc_test)]\n",
    "\n",
    "dictionary = corpora.Dictionary(all_doc_list)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in all_doc_list]\n",
    "\n",
    "doc_test_vec = dictionary.doc2bow(doc_test_list)\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary.keys()))\n",
    "sim = index[tfidf[doc_test_vec]]\n",
    "\n",
    "print(sorted(enumerate(sim), key=lambda item: -item[1]))\n",
    "```\n",
    "\n",
    "doc_sim.py\n",
    "\n",
    "```python\n",
    "#coding:utf-8\n",
    "#使用docsim方法：doc2bow、similarities判断相似性\n",
    "from gensim import models,corpora,similarities\n",
    "import jieba.posseg as pseg\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def a_sub_b(a,b):\n",
    "    ret = []\n",
    "    for el in a:\n",
    "        if el not in b:\n",
    "            ret.append(el)\n",
    "    return ret\n",
    "\n",
    "#读取文件\n",
    "raw_documents=[]\n",
    "walk = os.walk(os.path.realpath(\"/root/python_scripts/doc_similarity/doc/\"))\n",
    "for root, dirs, files in walk:\n",
    "    for name in files:\n",
    "        f = open(os.path.join(root, name), 'r')\n",
    "    raw = os.path.join(root, name)+ \" \"\n",
    "    raw += f.read()\n",
    "    raw_documents.append(raw)\n",
    "stop = [line.strip() for line in open('stopword.txt').readlines() ]\n",
    "#创建语料库\n",
    "corpora_documents = []\n",
    "for item_text in raw_documents:\n",
    "    item_str=[]\n",
    "    item= (pseg.cut(item_text)) #使用jieba分词\n",
    "    for i in list(item):\n",
    "        item_str.append(i.word)\n",
    "    item_str=a_sub_b(item_str,list(stop))\n",
    "    print(item_str)\n",
    "    #sys.exit(0)\n",
    "    corpora_documents.append(item_str)\n",
    "\n",
    "# 生成字典和向量语料\n",
    "dictionary = corpora.Dictionary(corpora_documents) #把所有单词取一个set，并对set中每一个单词分配一个id号的map\n",
    "corpus = [dictionary.doc2bow(text) for text in corpora_documents]  #把文档doc变成一个稀疏向量，[(0,1),(1,1)]表明id为0,1的词出现了1次，其他未出现。\n",
    "similarity = similarities.Similarity('-Similarity-index', corpus, num_features=999999999)\n",
    "\n",
    "test_data_1 = '本报讯 全球最大个人电脑制造商戴尔公司８日说，由于市场竞争激烈，以及定价策略不当，该公司今年第一季度盈利预计有所下降。'\\\n",
    "'消息发布之后，戴尔股价一度下跌近６％，创下一年来的新低。戴尔公司估计，其第一季度收入约为１４２亿美元，每股收益３３美分。此前公司预测当季收入为１４２亿至１４６亿美元，'\\\n",
    "'每股收益３６至３８美分，而分析师平均预测戴尔同期收入为１４５．２亿美元，每股收益３８美分。为抢夺失去的市场份额，戴尔公司一些产品打折力度很大。戴尔公司首席执行官凯文·罗林斯在一份声明中说，公司在售后服务和产品质量方面一直在投资，同时不断下调价格。戴尔公司将于５月１８日公布第一季度的财报。'\n",
    "test_cut = pseg.cut(test_data_1)\n",
    "test_cut_raw_1=[]\n",
    "for i in list(test_cut):\n",
    "    test_cut_raw_1.append(i.word)\n",
    "test_corpus_1 = dictionary.doc2bow(test_cut_raw_1)\n",
    "similarity.num_best = 5\n",
    "print(similarity[test_corpus_1])  # 返回最相似的样本材料,(index_of_document, similarity) tuples\n",
    "for i in similarity[test_corpus_1]:\n",
    "    sim=\"\"\n",
    "    print('################################')\n",
    "    print(i[0])\n",
    "    for j in corpora_documents[i[0]]:\n",
    "        sim+=j\n",
    "print(sim)\n",
    "```\n",
    "\n",
    "test_sim_1.py\n",
    "\n",
    "```python\n",
    "# coding: utf-8\n",
    "import jieba\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "doc0 = \"我不喜欢上海\"\n",
    "doc1 = \"上海是一个好地方\"\n",
    "doc2 = \"北京是一个好地方\"\n",
    "doc3 = \"上海好吃的在哪里\"\n",
    "doc6 = \"上海路和上海人\"\n",
    "doc7 = \"喜欢小吃\"\n",
    "doc_test=\"我喜欢上海的小吃\"\n",
    "\n",
    "all_document = []\n",
    "all_document.append(doc0)\n",
    "all_document.append(doc1)\n",
    "all_document.append(doc2)\n",
    "all_document.append(doc3)\n",
    "all_document.append(doc6)\n",
    "all_document.append(doc7)\n",
    "\n",
    "all_document_list = []\n",
    "for doc in all_document:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    all_document_list.append(doc_list)\n",
    "\n",
    "print(all_document_list)\n",
    "\n",
    "doc_test_l = [word for word in jieba.cut(doc_test)]\n",
    "\n",
    "dictionary = corpora.Dictionary(all_document_list)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in all_document_list]\n",
    "\n",
    "doc_test_vec = dictionary.doc2bow(doc_test_l)\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "index_value = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary.keys()))\n",
    "sim = index_value[tfidf[doc_test_vec]]\n",
    "\n",
    "print(sorted(enumerate(sim), key=lambda item: -item[1]))\n",
    "```\n",
    "\n",
    "运行结果：\n",
    "\n",
    "```\n",
    "(py3) ➜  doc_similarity python mydoc_sim.py\n",
    "Building prefix dict from the default dictionary ...\n",
    "Loading model from cache /tmp/jieba.cache\n",
    "Loading model cost 2.526 seconds.\n",
    "Prefix dict has been built succesfully.\n",
    "[(2, 0.9530888), (1, 0.070823014), (0, 0.010986276)]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
